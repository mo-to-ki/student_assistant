{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備\n",
    "## ライブラリのインストール\n",
    "Colaboratory環境で実行する場合は`japanaize_matplotlib`をインストールする必要があります。\n",
    "\n",
    "SageMaker環境で以下を実行する必要はありませんが、実行しても害はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install japanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b79J21iz8d4"
   },
   "source": [
    "## ライブラリのインポートとデータの読み込み\n",
    "\n",
    "この資料で使用するライブラリを一括してインポートし、必要なデータを読み込みます。\n",
    "\n",
    "SageMakerやColaboratoryの接続がタイムアウトした時は、\n",
    "以下のセルを再実行して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2511,
     "status": "ok",
     "timestamp": 1679962134584,
     "user": {
      "displayName": "大豆生田幹",
      "userId": "17706619674069057736"
     },
     "user_tz": -540
    },
    "id": "V6bMR59i0o5R",
    "outputId": "4010836c-2077-4cd4-96b3-9ffd5cd6b8b5"
   },
   "outputs": [],
   "source": [
    "# ライブラリーのインポート\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import japanize_matplotlib\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# データセットの読み出し\n",
    "akutagawa_name = np.load('data/akutagawa_name.npy', allow_pickle=True)\n",
    "kikuchi_name = np.load('data/kikuchi_name.npy', allow_pickle=True)\n",
    "hgram = np.load('data/hgram.npy', allow_pickle=True)\n",
    "words = np.load('data/words.npy', allow_pickle=True)\n",
    "print('芥川龍之介の作品は以下の20編')\n",
    "print(' '.join(akutagawa_name))\n",
    "print('\\n菊池寛の作品は以下の20編')\n",
    "print(' '.join(kikuchi_name))\n",
    "print('\\n40編の作品に現れる語の総数は{}語'.format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuzQEZ79QMrv"
   },
   "source": [
    "## 2.12 データの加工と類似度関数の設計(第3ラウンド)\n",
    "\n",
    "ここでは、類似度関数として、ユークリッド距離に替えて、マンハッタン距離を利用することで正解率の改善を目指します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3csXOSKwE01"
   },
   "source": [
    "### $L^1$ノルム\n",
    "類似度関数に$L^1$距離を使用するモチベーションについて述べます。はじめに、ベクトル$v,w$の$L^1$距離は以下で計算されます。\n",
    "\n",
    "$$\n",
    "d_{L1}((v_1, \\dots, v_{16301}), (w_1, \\dots, w_{16301})) = \n",
    "\\Vert (v_1, \\dots, v_{16301}) - (w_1, \\dots, w_{16301})\\Vert_1 =\n",
    "\\sum_{i=1}^{16301} \\vert v_i - w_i \\vert \n",
    "$$\n",
    "\n",
    "$L^1$ノルムと$L^2$ノルムの違いを以下のようにまとめることができます。\n",
    "\n",
    ">**$L^1$ノルムと$L^2$ノルムの比較**\n",
    ">\n",
    ">$L^1$ノルムと$L^2$ノルムを比較すると、相対的に次の傾向が見られます。\n",
    ">\n",
    ">*  $L^1$ノルムは、ベクトルが0に近い成分を多く持つ時、たとえ、\n",
    "その他の成分の絶対値が比較的大きい場合でも小さく評価する\n",
    ">*  $L^2$ノルムは、成分の絶対値が平均的に小さいベクトルを小さく評価する\n",
    "\n",
    "詳しくは「L2距離とL1距離.ipynb」を参照して下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文体に関する適用した時の$L^2$ノルムとの違い\n",
    "\n",
    "作品に現れる語彙には次の2種類があると考えられます。\n",
    "\n",
    "1. **作品に固有の語彙**.\n",
    "同一の著者による作品の間でも、作品毎に作品中での出現確率が大きく異なることが想定されます。\n",
    "1. **作品によらず普遍的に現れる語彙**.\n",
    "著者の文体は、この語彙の単語の作品中での出現確率によって特徴付けられる、\n",
    "即ち、この語彙の単語の分布は著者によって一定の傾向を示すことが想定される。\n",
    "\n",
    "言い換えると、$\\vec v_1$と$\\vec v_2$を、\n",
    "同一の著者の2編の作品に対する単語の出力確率を表すベクトル（確率分布）とします。\n",
    "この時、$\\vec v_1$と$\\vec v_2$の差$\\vec v_1 - \\vec v_2$の成分について考察しましょう。\n",
    "\n",
    "1. **作品に固有の語彙**の単語では、$\\vec v_1 - \\vec v_2$の成分は0から遠い値になります。\n",
    "1. **作品によらず普遍的に現れる語彙**の単語では、$\\vec v_1 - \\vec v_2$の成分は0に近い値になります。\n",
    "\n",
    "つまり、同じ著者による作品の確率分布ベクトル$\\vec v_1 - \\vec v_2$の長さ（ノルム）は、\n",
    "$L^1$ノルムを用いた方が$L^2$ノルムを用いた場合よりも小さく評価され、\n",
    "**同一の著者の作品をより近く、異なる著者の作品をより遠く**評価されること分かります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n_S6JsF3Yld"
   },
   "source": [
    "### $L^1$正規化と確率分布\n",
    "今回は類似度関数に$L^1$距離(マンハッタン距離)を使用することにしたため、正規化でも$L^1$正規化を行うことにします。\n",
    "\n",
    "$$\n",
    "  \\frac{\\vec v}{\\Vert\\vec v\\Vert_1} =\n",
    "  \\left(\\frac{v_1}{\\vert v_1\\vert + \\dots + \\vert v_{16301}\\vert}, \\dots,\n",
    "    \\frac{v_{16301}}{\\vert v_1\\vert + \\dots + \\vert v_{16301}\\vert}\\right)\n",
    "$$\n",
    "\n",
    "$L^1$正規化で得られる値は語の出現確率になります。\n",
    "\n",
    ">- 全ての成分は0以上で、総和は1になるからです。\n",
    ">- 以下のコードでは`hgrm`の要素を$L_1$正規化して、リスト`prob`を生成します。\\\n",
    "`hgrm[i]`の頻度情報を$L^1$正規化した値は`prob[i]`に保存します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1y9cXcmi3Wms"
   },
   "outputs": [],
   "source": [
    "def dl1(dictx, dicty): # L1距離\n",
    "    sq = 0\n",
    "    for key in set(dictx.keys()) | set(dicty.keys()):\n",
    "        x, y = 0, 0\n",
    "        if key in dictx: \n",
    "            x = dictx[key]\n",
    "        if key in dicty:\n",
    "            y = dicty[key]\n",
    "        sq += abs(x - y)\n",
    "    return sq\n",
    "\n",
    "def distribution(dct): # ヒストグラムを確率分布に変換\n",
    "    s = sum(dct.values())\n",
    "    return dict([(k, v/s) for k, v in dct.items()]) \n",
    "\n",
    "prob = list(map(lambda dct: distribution(dct), hgram)) # 確率分布データ\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7s1JYNl5q1o"
   },
   "source": [
    "「蜘蛛の糸」を例に、$L^2$正規化後の分布と、$L^1$正規化後の分布を比較してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ej-KB5Mk3dzz"
   },
   "outputs": [],
   "source": [
    "def l2normalize(dct): # L2正規化\n",
    "    s = np.sqrt(sum([v**2 for v in dct.values()]))\n",
    "    return dict([(k, v/s) for k, v in dct.items()]) \n",
    "\n",
    "hgrm_nrm = [l2normalize(dct) for dct in hgram] # hgramは前に作成したリスト。各作品の単語の種類とその単語の出現回数を持つ辞書型オブジェクトを作品の数だけ持つ。\n",
    "\n",
    "def countl2(n, w): # n番目の作品にwという単語が何回出現しているかを返す関数\n",
    "    if w in hgrm_nrm[n]:\n",
    "        return hgrm_nrm[n][w]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def countl1(n, w): # n番目の作品において、無作為に名詞・動詞・形容詞・副詞の中から一つ単語を抽出したときwという単語である確率を返す関数\n",
    "    if w in prob[n]:\n",
    "        return prob[n][w]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# L1正規化とL2正規化の分布の比較\n",
    "_, axes = plt.subplots(1, 2, figsize=[15,5])\n",
    "\n",
    "y = [countl2(19, w) for w in words] # 19は蜘蛛の糸に対応\n",
    "axes[0].plot(range(len(words)), y)\n",
    "axes[0].set_title(\"蜘蛛の糸($L_2$正規化)\")\n",
    "axes[0].set_ylim([0, 0.3])\n",
    "axes[0].xaxis.set_visible(False)\n",
    "\n",
    "y = [countl1(19, w) for w in words]\n",
    "axes[1].plot(range(len(words)), y)\n",
    "axes[1].set_title(\"蜘蛛の糸($L_1$正規化)\")\n",
    "axes[1].set_ylim([0, 0.3])\n",
    "axes[1].xaxis.set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icySi_TD3e-G"
   },
   "source": [
    "前回のサイクルと同様に次元圧縮後、3次元表示して、感触をつかみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lLHsRiBKC7A2"
   },
   "outputs": [],
   "source": [
    " # L1距離を用いた距離行列の作成とMDSによる圧縮&可視化\n",
    "temp = [] \n",
    "for dictx in prob:\n",
    "    temp.append(list(map(lambda dicty: dl1(dictx, dicty), prob)))\n",
    "dl1_matrix = np.array(temp) # 距離の行列\n",
    "\n",
    "dl1_matrix\n",
    "\n",
    "mds = manifold.MDS(n_components=3, dissimilarity=\"precomputed\", random_state=6)\n",
    "pos = mds.fit_transform(dl1_matrix)\n",
    "\n",
    "l1 = len(akutagawa_name)\n",
    "l2 = len(kikuchi_name)\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=go.Layout(\n",
    "    title=\"L1正規化後、マンハッタン距離に基づくデータの分布(MDSで次元圧縮)\",\n",
    "    showlegend=True,\n",
    "    legend=dict(x=0.7, y=0.99, xanchor='left', yanchor='top', font=dict(size=16))\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "        x = pos[0: l1,0], y = pos[0: l1,1], z = pos[0: l1,2],\n",
    "        mode = \"markers\",\n",
    "        marker = dict(symbol=\"cross\", color=\"black\", size=5, \n",
    "                      line=dict(width=0), opacity=1 ),\n",
    "        name = \"芥川\"\n",
    "        )\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter3d(\n",
    "        x = pos[l1: l1+l2, 0], y = pos[l1: l1+l2, 1], z = pos[l1: l1+l2, 2],\n",
    "        mode = \"markers\",\n",
    "        marker = dict(symbol=\"circle\", color=\"black\", size=5, \n",
    "                      line=dict(width=0), opacity=1),\n",
    "        name = \"菊池\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzuw7RewQVAG"
   },
   "source": [
    "分布が明確に分離し、分布の間の間隙もはっきりしているように見えます。\n",
    "つまり、k-NNによる分類に適した分布の様相を示しており、期待ができます。\n",
    "\n",
    "## 2.13 学習アルゴリズムの選択(第3ラウンド)\n",
    "2.12節でのMDSの結果を見ると、k-NNアルゴリズムでの正解率の向上が期待できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be_Mg8KMQb4e"
   },
   "source": [
    "## 2.14 ハイパーパラメータの最適化とモデルの評価(第3ラウンド)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iqPw7k_b3sc4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "labels = [0]*l1 + [1]*l2\n",
    "\n",
    "parameters = [{\"metric\": [\"precomputed\"], \"n_neighbors\": list(range(1,11))}]\n",
    "clf = GridSearchCV(KNeighborsClassifier(), parameters, cv=5)\n",
    "clf.fit(dl1_matrix, labels)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "params = clf.cv_results_[\"params\"]\n",
    "mean_test_score = clf.cv_results_[\"mean_test_score\"]\n",
    "std_test_score = clf.cv_results_[\"std_test_score\"]\n",
    "for p, m, s in zip(params, mean_test_score, std_test_score):\n",
    "    print(f\"{m:.3f} (+/- {s/2:.3f}) for {p}\")\n",
    "    x.append(p['n_neighbors'])\n",
    "    y.append(m)\n",
    "\n",
    "\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel(\"$k$\", fontsize=16)\n",
    "plt.ylabel(\"正解率\", fontsize=16)\n",
    "plt.bar(x, y)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elbuTGev3ufi"
   },
   "source": [
    "$k$の最適値と、正解率の最大値は以下の通りです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "64iQFAMh7o_d"
   },
   "outputs": [],
   "source": [
    "print(f\"kの最適値:\\t{clf.best_estimator_.n_neighbors}\")\n",
    "scores = model_selection.cross_val_score(clf.best_estimator_, X = dl1_matrix, y = labels, cv = 5)\n",
    "print(f\"k={clf.best_estimator_.n_neighbors}での正解率:\\t{np.average(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhddfNfT7s4J"
   },
   "source": [
    "良好な正解率を得ることができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXKCNiPDQlwx"
   },
   "source": [
    "## 2.15 まとめ\n",
    "\n",
    "芥川龍之介の作品と菊池寛の作品をテキストのみに基づいて判別する簡単な「人工知能」を作成しました。\n",
    "\n",
    "- 「自立語である名詞・動詞・形容詞・副詞の分布が著者の文体を特徴付ける」という仮説に基づく。\n",
    "- データをベクトルと考え、ノルム（（$L^2$ノルム・$L^1$ノルム）による類似度関数を試した。\n",
    "- データを正規化（$L^2$正規化・$L^1$正規化）することで、作品の長さが与える悪影響を除去することを狙った。\n",
    "- 結果、$L^1$ノルムによる類似度関数、データの$L^1$正規化、k-NNアルゴリズムの利用により、予測の正解率を満足できるレベルに高めることができることを発見した。\n",
    "\n",
    "高い正解率に到達するまでに、問題を考察しつつ、試行錯誤による探索を行った点が重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LFwIAxT07ueB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hzuw7RewQVAG"
   ],
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
